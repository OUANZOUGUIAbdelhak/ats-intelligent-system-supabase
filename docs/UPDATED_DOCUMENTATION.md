# ATS Intelligent System — Documentation

> Applicant Tracking System with OCR, LLM structuring, semantic search — Supabase-native.

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Architecture](#2-architecture)
3. [Data Models](#3-data-models)
4. [Storage & Security](#4-storage--security)
5. [Data Display](#5-data-display)
6. [APIs](#6-apis)
7. [Deployment](#7-deployment)

---

## 1. Introduction

### What is this?

The **ATS Intelligent System** is an Applicant Tracking System that:

- Extracts text from CVs (PDF, DOCX, images) via **Docling OCR**
- Structures CVs using **Groq LLM** (flexible JSON)
- Generates **384-dim embeddings** (sentence-transformers all-MiniLM-L6-v2)
- Stores everything in **Supabase** (Postgres + pgvector + Storage)
- Supports **semantic search** and **candidate scoring**

### Stack

| Layer | Technology |
|-------|------------|
| Backend | FastAPI, Python 3.11+ |
| Frontend | React 18, TypeScript, Vite, Tailwind |
| Database | Supabase Postgres + pgvector |
| Storage | Supabase Storage (private bucket) |
| Auth | Supabase Auth (optional; demo uses fixed user) |
| OCR | Docling |
| LLM | Groq (Llama) |
| Embeddings | sentence-transformers all-MiniLM-L6-v2 |

---

## 2. Architecture

### Single Backend + Supabase

```
┌─────────────────────────────────────────────────────────────┐
│                     ATS Intelligent System                    │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│   Frontend (React + Vite)                                     │
│   ├── Supabase JS (anon key) – auth, optional direct DB      │
│   └── Backend API (axios) – CV ingest, search, matching      │
│                          │                                    │
│                          ▼                                    │
│   Backend (FastAPI)                                           │
│   ├── Supabase client (service_role) – full access             │
│   ├── OCR (Docling) → raw_text                                │
│   ├── LLM (Groq) → structured_data                            │
│   ├── Embedding → vector(384)                                  │
│   └── Storage → cv-originals bucket                           │
│                          │                                    │
│                          ▼                                    │
│   Supabase                                                    │
│   ├── Postgres (cv_documents, pgvector)                       │
│   ├── Storage (cv-originals, private)                         │
│   └── Auth (email/password, magic link)                       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Pipeline Flow

1. **Ingest**: `POST /api/cv/ingest` (multipart file)
2. Upload file → Supabase Storage (`cv-originals/{user_id}/{cv_id}/{filename}`)
3. OCR (Docling) → `raw_text`
4. LLM (Groq) → `structured_data` (JSON)
5. Embedding → `vector(384)`
6. Insert row into `cv_documents`

---

## 3. Data Models

### cv_documents Table

| Column | Type | Description |
|--------|------|-------------|
| id | uuid | Primary key |
| user_id | uuid | Owner (auth.users) |
| original_file_path | text | Storage path |
| raw_text | text | OCR output |
| structured_data | jsonb | Flexible CV structure |
| embedding | vector(384) | Semantic embedding |
| quality_score | float | 0–1 quality |
| status | text | pending, processing, active, error |
| original_filename | text | Original file name |
| mime_type | text | MIME type |
| file_size_bytes | bigint | File size |
| gdpr_consent | boolean | GDPR consent flag |
| created_at, updated_at | timestamptz | Timestamps |

### structured_data (JSONB)

Flexible structure from the LLM:

```json
{
  "candidate_info": {
    "full_name": "...",
    "email": "...",
    "phone": "...",
    "location": "...",
    "summary": "..."
  },
  "sections": [
    {
      "section_type": "experience",
      "section_title": "Experience",
      "content": { "experiences": [...] }
    }
  ],
  "experiences": [...],
  "education": [...],
  "skills": [...],
  "career_summary": { "years_of_experience": 5, "seniority_level": "senior" }
}
```

---

## 4. Storage & Security

### How Files Are Stored

- **Bucket**: `cv-originals` (private)
- **Path**: `{user_id}/{cv_id}/{filename}`
- **Access**: Only via **signed URLs** generated by the backend (service role)
- **Expiry**: 1 hour (configurable via `SIGNED_URL_EXPIRY`)

### RLS Policies (SQL)

**cv_documents table:**

```sql
-- Users see only their own CVs
CREATE POLICY "Users can view own CVs"
    ON cv_documents FOR SELECT
    USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own CVs"
    ON cv_documents FOR INSERT
    WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can update own CVs"
    ON cv_documents FOR UPDATE
    USING (auth.uid() = user_id);

CREATE POLICY "Users can delete own CVs"
    ON cv_documents FOR DELETE
    USING (auth.uid() = user_id);
```

**storage.objects (cv-originals):**

```sql
CREATE POLICY "Users can upload own CVs"
    ON storage.objects FOR INSERT
    WITH CHECK (
        bucket_id = 'cv-originals'
        AND auth.role() = 'authenticated'
        AND (storage.foldername(name))[1] = auth.uid()::text
    );

CREATE POLICY "Users can read own CVs"
    ON storage.objects FOR SELECT
    USING (
        bucket_id = 'cv-originals'
        AND auth.role() = 'authenticated'
        AND (storage.foldername(name))[1] = auth.uid()::text
    );
```

### Auth Flow

1. Frontend uses Supabase Auth (email/password or magic link)
2. User gets JWT; frontend passes it to backend (future: `Authorization: Bearer <jwt>`)
3. Backend uses **service_role** for DB/storage — bypasses RLS
4. Backend enforces `user_id` from JWT for filtering (to be wired in production)

### PII / GDPR Notes

- `gdpr_consent` stored per CV
- `raw_text` and `structured_data` may contain PII
- Consider encryption-at-rest, retention policies, anonymization workflows
- Audit trail can be added via `audit_trail` in structured_data or a separate table

---

## 5. Data Display

### How the Frontend Gets Data

1. **Signed URL**: `GET /api/cv/{id}` returns `signed_url` — frontend uses it in `<iframe>` or `<a download>` for the original document
2. **Raw text**: `raw_text` from API response — displayed in a `<pre>` block
3. **Structured JSON**: `structured_data` — rendered as sections (skills, experience, etc.) and a JSON viewer
4. **Embedding**: `embedding_preview` — shows dimension and first 5 values

### CV Viewer Layout

- Header: name, file, status, download link (signed URL)
- Candidate info: email, phone, location
- Skills (chips)
- Experience (timeline)
- Raw OCR text (collapsible)
- Structured JSON (collapsible)
- Embedding preview (dimension + sample)

---

## 6. APIs

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/cv/ingest` | Upload CV, run full pipeline |
| GET | `/api/cv/{id}` | Get CV + signed URL, raw_text, structured, embedding |
| GET | `/api/cv/search` | List or semantic search (?q=...) |
| DELETE | `/api/cv/{id}` | Delete CV and storage file |
| POST | `/api/matching/semantic` | Semantic matching (job_description, top_n) |
| POST | `/api/scoring/candidates` | Score candidates by criteria |
| GET | `/api/demo/load` | Load 4 demo CVs into DB |

---

## 7. Deployment

### Connect to Your Supabase Project

1. Create a project at [supabase.com](https://supabase.com)
2. In **Project Settings → API**: copy `Project URL`, `anon key`, `service_role key`
3. In **SQL Editor**: run `supabase/migrations/001_initial.sql`
4. Create bucket `cv-originals` if not created by migration (check Storage)

### Environment Variables

**Backend (.env):**

```
SUPABASE_URL=https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY=eyJ...
SUPABASE_ANON_KEY=eyJ...
GROQ_API_KEY=gsk_...
```

**Frontend (.env):**

```
VITE_API_URL=https://your-backend.railway.app
VITE_SUPABASE_URL=https://xxxx.supabase.co
VITE_SUPABASE_ANON_KEY=eyJ...
```

### Run Locally

```bash
# Backend
cd backend
python -m venv venv
venv\Scripts\activate   # Windows
pip install -r requirements.txt
cp .env.example .env   # fill in values
uvicorn app.main:app --reload --port 8000

# Frontend (new terminal)
cd frontend
npm install
npm run dev
```

### Deploy

- **Backend**: Railway, Render, Fly.io — set env vars, run `uvicorn app.main:app --host 0.0.0.0 --port 8000`
- **Frontend**: Vercel, Netlify — set `VITE_API_URL` and Supabase vars, build with `npm run build`

---

## Demo

1. Open the app → go to **Demo** page
2. Click **Load Demo Data**
3. Four CVs are processed: clean PDF-style, scanned, messy, multilingual
4. View them under **CVs** or use **Matching** to search semantically

---

*Documentation — ATS Intelligent System — February 2025*
